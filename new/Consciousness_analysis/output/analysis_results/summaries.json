[
  {
    "filename": "s41467-024-49173-5.pdf",
    "file_id": "file-CNfnnY2uYNz6k2NmWBGAaW",
    "vector_store_id": "vs_68b8a65056bc819185b5ae74c8d28b4a",
    "size_mb": 5.43,
    "summary": "Figure 1 introduces the study’s core workflow (how features are used to predict brain activity) and what, exactly, the authors extract from Transformer language models to do so.\n\nPanel A: Encoding setup and evaluation\n- What’s shown: A pipeline from text transcripts to brain predictions, with a training/testing loop and a correlation-based evaluation.\n- Inputs/features: The authors extract three families of predictors from the story transcripts heard during fMRI: (1) classical linguistic features (e.g., parts of speech, syntactic dependencies), (2) non‑contextual word embeddings (GloVe), and (3) internal features from Transformer models (BERT-base; GPT‑2 is used for replications elsewhere) .\n- Model fitting: Encoding models are estimated with banded ridge regression on a training subset and then evaluated on a left‑out test portion using three‑fold cross‑validation .\n- Metric: Performance is the correlation r between the predicted and actual fMRI time series on the held‑out data (the figure shows “predicted” vs “actual” and the computed r) .\n\nPanel B: The three Transformer-derived feature types\n- Embeddings (blue): At each layer, these represent the contextualized semantic content of tokens. BERT-base and GPT‑2 each have 12 layers; each layer’s embedding is 768‑dimensional in BERT-base .\n- Transformations (red): For each layer, the model’s 12 attention heads each produce a 64‑dimensional “transformation” vector; concatenating the 12 heads yields 768 dims per layer. These transformations are the outputs of self‑attention that inject context into a token at that layer—i.e., the incremental update added to the residual stream before the feed‑forward MLP fuses it into the next-layer embedding .\n- Transformation magnitudes (yellow): A direction‑free measure—L2 norms of each head’s 64‑dimensional transformation—summarizing how “active” each head was for a token, without encoding semantic directionality .\n- Architectural context the figure relies on: In each layer, the 12 heads run in parallel and their 64‑D outputs are concatenated (12×64=768), then passed to a feed‑forward MLP; residual connections add the layer’s transformation into the running embedding (the “residual stream”) that carries forward across layers  .\n\nPanel C: How a single attention head updates a token\n- Setup in the schematic: A specific example focuses on the token “plan,” passing through one head (layer 7, head 12) to illustrate the computation depicted in the panel .\n- Step 1 (Q, K, V): The token vector is multiplied by the head’s learned weight matrices to produce query (Q), key (K), and value (V) vectors. These matrices are fixed for the head but applied to any input token .\n- Step 2 (attention weights): The head computes attention weights by comparing the token’s Q to all tokens’ K (softmax of scaled dot products), yielding a distribution over context tokens that reflects their relevance to “plan” .\n- Step 3 (transformation output): The head forms a weighted sum of the V vectors using those attention weights; this 64‑D result is the head’s transformation for “plan” (z for that head). All heads’ transformations at the layer are concatenated and then added back to the token’s existing representation in the residual stream before the MLP produces the updated embedding for the next layer  .\n- Intuition the figure conveys: Attention functions like a soft key‑value store for context—e.g., in a phrase like “hatch the secret plan,” the “plan” token can attend strongly to “hatch,” pulling in vhatch to update its meaning to reflect that “plan” is being hatched. This illustrates how specific heads can approximate particular syntactic relations while adding context-sensitive adjustments to a token’s representation .\n\nOverall, Figure 1 defines the prediction task (A) and clarifies the three Transformer-based feature types used to model brain activity (B), grounding them in the head-level self‑attention computation that produces the contextual “transformations” (C)  ."
  }
]